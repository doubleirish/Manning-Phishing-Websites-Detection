{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 11055 rows with 31 Columns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Fix the random seed\n",
    "np.random.seed(7)\n",
    "# Identify  how many rows and columns we may need to clean \n",
    "path=  \"./Phishing.csv\"  \n",
    "data=pd.read_csv(path)    \n",
    "(rows, cols) = data.shape\n",
    "print (\"Read\", rows, \"rows with\", cols, \"Columns\")\n",
    "data.rename(columns={\"Result\": \"Class\"}, inplace=True)\n",
    "\n",
    "data[\"Class\"] = data[\"Class\"].map({-1:0, 1:1})\n",
    "data[\"Class\"].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "having_IP_Address              0\n",
       "URL_Length                     0\n",
       "Shortining_Service             0\n",
       "having_At_Symbol               0\n",
       "double_slash_redirecting       0\n",
       "Prefix_Suffix                  0\n",
       "having_Sub_Domain              0\n",
       "SSLfinal_State                 0\n",
       "Domain_registeration_length    0\n",
       "Favicon                        0\n",
       "port                           0\n",
       "HTTPS_token                    0\n",
       "Request_URL                    0\n",
       "URL_of_Anchor                  0\n",
       "Links_in_tags                  0\n",
       "SFH                            0\n",
       "Submitting_to_email            0\n",
       "Abnormal_URL                   0\n",
       "Redirect                       0\n",
       "on_mouseover                   0\n",
       "RightClick                     0\n",
       "popUpWidnow                    0\n",
       "Iframe                         0\n",
       "age_of_domain                  0\n",
       "DNSRecord                      0\n",
       "web_traffic                    0\n",
       "Page_Rank                      0\n",
       "Google_Index                   0\n",
       "Links_pointing_to_page         0\n",
       "Statistical_report             0\n",
       "Class                          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We can look for missing values in the dataset like so -\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DF     has  8844 rows with 30 Columns\n",
      "Test     DF     has  2211 rows with 30 Columns\n"
     ]
    }
   ],
   "source": [
    "# split up into training an test sets . \n",
    "# Let's now split the dataset in a 80:20 ratio. \n",
    " \n",
    "from sklearn.model_selection import train_test_split\n",
    " \n",
    "X = data.iloc[:,0:30].values.astype(int)\n",
    "y = data.iloc[:,30].values.astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=np.random.seed(7))\n",
    "\n",
    "\n",
    "print (\"Training DF     has \", X_train.shape[0], \"rows with\", X_train.shape[1], \"Columns\")\n",
    "print (\"Test     DF     has \", X_test.shape[0], \"rows with\", X_test.shape[1], \"Columns\")\n",
    "#print (\"Training Result has \", len(y_train), \"results \")\n",
    "#print (\"Test     Result has \", len(y_test), \"results \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Serialize the numpy arrays\n",
    "np.save(\"X_train.npy\", X_train), np.save(\"y_train.npy\", y_train)\n",
    "np.save(\"X_test.npy\", X_train), np.save(\"y_test.npy\", y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a logistic regression model, and fit with X and y\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model = model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted [0 0 0 ... 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "predicted = model.predict(X_test)\n",
    "print(\"predicted\" , predicted)\n",
    "# check the accuracy on the training set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.97743414e-01 2.25658600e-03]\n",
      " [8.13428411e-01 1.86571589e-01]\n",
      " [7.67475752e-01 2.32524248e-01]\n",
      " ...\n",
      " [9.94115162e-01 5.88483766e-03]\n",
      " [1.58703772e-01 8.41296228e-01]\n",
      " [9.99856616e-01 1.43384102e-04]]\n"
     ]
    }
   ],
   "source": [
    "model.score(X_train, y_train)\n",
    "probs = model.predict_proba(X_test) \n",
    "print (probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score 0.9371325192220714\n",
      "roc_auc_score 0.9839028151502526\n"
     ]
    }
   ],
   "source": [
    "# generate evaluation metrics\n",
    "from sklearn import metrics\n",
    "print (\"accuracy score\",metrics.accuracy_score(y_test, predicted))\n",
    "print(\"roc_auc_score\",metrics.roc_auc_score(y_test, probs[:, 1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion_matrix\n",
      " [[ 896   78]\n",
      " [  61 1176]]\n",
      "classification_report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.92      0.93       974\n",
      "           1       0.94      0.95      0.94      1237\n",
      "\n",
      "    accuracy                           0.94      2211\n",
      "   macro avg       0.94      0.94      0.94      2211\n",
      "weighted avg       0.94      0.94      0.94      2211\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# confusion_matrix to see how predictins compared with the real results \n",
    "from sklearn import metrics\n",
    "# True Positives (good)| False Positives (bad)\n",
    "# False Negatives (bad)| true Negatives(good)\n",
    " \n",
    "print(\"confusion_matrix\\n\",metrics.confusion_matrix(y_test, predicted))\n",
    "# classification \n",
    "print(\"classification_report\\n\",metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': (0.8, 0.9, 1.0), 'penalty': ['l1', 'l2'], 'tol': [0.01, 0.001, 0.0001], 'max_iter': [100, 150, 200, 250]}\n",
      "expected BestPenalty:   l1\n",
      "expected Best C:        1.0\n",
      "expected Best tol:      0.001\n",
      "expected Best max_iter: 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\credmond\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\model_selection\\_search.py:281: UserWarning: The total space of parameters 72 is smaller than n_iter=100. Running 72 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Penalty: l2\n",
      "Best C:       0.9\n",
      "Best tol:     0.01\n",
      "Best max_iter 100\n"
     ]
    }
   ],
   "source": [
    "# try to find a better fit using RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "import numpy as np\n",
    "\n",
    "from time import time\n",
    "import scipy.stats as stats\n",
    "from sklearn.utils.fixes import loguniform\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    " \n",
    "\n",
    "# Define the grid of values\n",
    "penalty = ['l1', 'l2'] \n",
    "C = (0.8, 0.9, 1.0) \n",
    "tol = [0.01, 0.001 ,0.0001]\n",
    "max_iter = [100, 150, 200, 250]\n",
    "# Create hyperparameter options\n",
    "hyperparameters = dict(C=C, penalty=penalty,tol=tol,max_iter=max_iter)\n",
    "print(hyperparameters)\n",
    "# Create randomized search 5-fold cross validation and 100 iterations\n",
    "clf = RandomizedSearchCV(model, hyperparameters, random_state=1, n_iter=100, cv=5, verbose=0, n_jobs=-1)\n",
    "\n",
    "\n",
    "#\n",
    "print('expected BestPenalty:   l1')\n",
    "print('expected Best C:        1.0')\n",
    "print('expected Best tol:      0.001')\n",
    "print('expected Best max_iter: 250')\n",
    "\n",
    "# Fit randomized search\n",
    "best_model = clf.fit(X_train, y_train)\n",
    "# View Hyperparameter Values Of Best Model\n",
    "\n",
    " \n",
    "# View best hyperparameters\n",
    "print('Best Penalty:', best_model.best_estimator_.get_params()['penalty'])\n",
    "print('Best C:      ', best_model.best_estimator_.get_params()['C'])\n",
    "print('Best tol:    ', best_model.best_estimator_.get_params()['tol'])\n",
    "print('Best max_iter', best_model.best_estimator_.get_params()['max_iter'])\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion_matrix...\n",
      " [[ 896   78]\n",
      " [  61 1176]]\n",
      "classification_report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.92      0.93       974\n",
      "           1       0.94      0.95      0.94      1237\n",
      "\n",
      "    accuracy                           0.94      2211\n",
      "   macro avg       0.94      0.94      0.94      2211\n",
      "weighted avg       0.94      0.94      0.94      2211\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# these results unfortunately do not match what was predicted in the exercise \n",
    "from sklearn import metrics\n",
    "predicted = best_model.predict(X_test)\n",
    "print(\"confusion_matrix...\\n\",metrics.confusion_matrix(y_test, predicted))\n",
    "# classification \n",
    "print(\"classification_report\\n\",metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/credmond/my-project\" target=\"_blank\">https://app.wandb.ai/credmond/my-project</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/credmond/my-project/runs/ydu32xro\" target=\"_blank\">https://app.wandb.ai/credmond/my-project/runs/ydu32xro</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# save best params  to wandb\n",
    "import wandb\n",
    "\n",
    "wandb.init(project=\"my-project\")\n",
    "wandb.log({'C': best_model.best_estimator_.get_params()['C']\n",
    "           , 'penalty': best_model.best_estimator_.get_params()['penalty']\n",
    "          , 'tol' : best_model.best_estimator_.get_params()['tol']\n",
    "          , 'max_iter' : best_model.best_estimator_.get_params()['max_iter']}) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
